# 低方差奖励的数学优势：策略梯度方差分析

**Authors:** Damon Li

## 1. 策略梯度定理回顾

策略梯度方法（如 REINFORCE）的目标是最大化期望回报 $J(\theta)$：
$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [G(\tau)]$$
其中 $\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \dots)$ 是轨迹，$G(\tau)$ 是总回报（例如，折扣累积奖励）。

策略梯度定理给出了梯度的无偏估计：
$$\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^{T} \nabla \log \pi_\theta(a_t|s_t) \right) G(\tau) \right]$$

在实际应用中，我们使用**单条轨迹** $\tau$ 上的样本来估计梯度 $\hat{g}$：
$$\hat{g} = \left( \sum_{t=0}^{T} \nabla \log \pi_\theta(a_t|s_t) \right) G(\tau)$$

## 2. 策略梯度估计的方差分析

我们关注的是估计量 $\hat{g}$ 的方差 $\text{Var}[\hat{g}]$。方差越小，训练越稳定，收敛越快。

为了简化分析，我们考虑一个单步 MDP，此时 $\hat{g} = \nabla \log \pi_\theta(a|s) R(s, a)$，其中 $R(s, a)$ 是即时奖励。

$$\text{Var}[\hat{g}] = \mathbb{E}[\hat{g}^2] - (\mathbb{E}[\hat{g}])^2$$

由于 $\mathbb{E}[\hat{g}] = \nabla J(\theta)$ 是一个常数（对于给定的 $\theta$），我们主要关注 $\mathbb{E}[\hat{g}^2]$。

$$\mathbb{E}[\hat{g}^2] = \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} \left[ (\nabla \log \pi_\theta(a|s))^2 R(s, a)^2 \right]$$

## 3. 低方差奖励对策略梯度方差的影响

我们假设 $\nabla \log \pi_\theta(a|s)$ 是一个与奖励 $R(s, a)$ **独立**的随机变量（虽然在实际中它们通过 $\pi_\theta$ 间接相关，但这个假设有助于理解核心机制）。

我们引入一个关键的数学不等式：对于任何随机变量 $X$ 和 $Y$，$\mathbb{E}[X^2 Y^2] \ge \mathbb{E}[X^2] \mathbb{E}[Y^2]$（当 $X, Y$ 独立时等号成立）。

我们关注奖励项 $R(s, a)^2$。

### 3.1 奖励方差与策略梯度方差的关系

我们定义奖励的**二阶矩** $M_2 = \mathbb{E}_{s, a} [R(s, a)^2]$。

$$\mathbb{E}[\hat{g}^2] \propto \mathbb{E}_{s, a} [(\nabla \log \pi_\theta(a|s))^2 R(s, a)^2]$$

如果我们将 $\nabla \log \pi_\theta(a|s)$ 视为一个权重项 $W(s, a)$，则：
$$\mathbb{E}[\hat{g}^2] \propto \mathbb{E}_{s, a} [W(s, a) R(s, a)^2]$$

奖励 $R(s, a)$ 的方差 $\text{Var}[R]$ 可以表示为：
$$\text{Var}[R] = \mathbb{E}[R^2] - (\mathbb{E}[R])^2 = M_2 - (\mathbb{E}[R])^2$$

因此，**奖励的二阶矩 $M_2$ 随着奖励方差 $\text{Var}[R]$ 的增大而增大**。

### 3.2 RLVR 的优势：最小化 $M_2$

RLVR 采用**可验证奖励**，通常是二元奖励 $R \in \{0, 1\}$。

1. **RLVR 奖励**：$R_{RLVR} \in \{0, 1\}$。
   - 假设 $P(R=1) = p$。
   - $\mathbb{E}[R_{RLVR}] = p$
   - $\mathbb{E}[R_{RLVR}^2] = 1^2 \cdot p + 0^2 \cdot (1-p) = p$
   - $\text{Var}[R_{RLVR}] = p - p^2 = p(1-p)$

2. **RLHF 奖励**：$R_{RLHF}$ 是奖励模型输出的连续值，通常具有较大的方差。
   - $\mathbb{E}[R_{RLHF}^2] = \text{Var}[R_{RLHF}] + (\mathbb{E}[R_{RLHF}])^2$

由于 RLVR 的奖励 $R_{RLVR}$ 仅限于 $\{0, 1\}$，其**二阶矩 $M_2$ 被天然限制在一个较小的范围内**（最大为 1）。相比之下，RLHF 的奖励 $R_{RLHF}$ 可以是任意大的连续值，导致其二阶矩 $M_2$ 远大于 RLVR，从而**显著增大了策略梯度估计的方差 $\text{Var}[\hat{g}]$**。

**结论**：
RLVR 的可验证奖励具有**低方差**和**有界性**，这直接导致了策略梯度估计的二阶矩 $\mathbb{E}[\hat{g}^2]$ 减小，从而**降低了策略梯度估计的方差**。

$$\text{Var}[\hat{g}] \propto \mathbb{E}[R^2] \downarrow \implies \text{Var}[\hat{g}] \downarrow$$

## 4. 实际意义

- **数值稳定性**：梯度估计的方差减小，避免了训练过程中的剧烈震荡和梯度爆炸，提高了数值稳定性。
- **收敛速度**：在相同的样本效率下，方差更小的梯度估计能够更快地收敛到最优策略。
- **样本效率**：由于梯度估计更准确，RLVR 可以使用更小的批量大小（Batch Size）或更少的环境交互次数来达到相同的训练效果。

## 参考文献

1. Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
2. Schulman, J., et al. (2015). *High-dimensional continuous control using generalized advantage estimation*. ICLR.
