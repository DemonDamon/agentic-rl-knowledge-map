# 3. 基座模型与模板工程

**Authors:** Damon Li

---

在 RL for LLM 的实践中，选择合适的基座模型和正确处理其输入/输出模板，是决定训练成败的、常常被忽视的关键环节。本节将深入探讨不同基座模型的 RL 适应性，以及在“思维链”(Thinking/Reasoning) 模型后训练中遇到的复杂模板工程挑战。

## 3.1. 基座模型的选择

并非所有预训练模型都同样适合进行 RL 后训练，尤其是对于需要复杂推理能力的任务。

-   **推荐模型**: 实践表明，**Qwen2.5** 和 **Mistral** 系列的 Instruct 或 Base 模型是进行 RL 后训练的良好起点。这些模型在预训练和指令微调阶段已经表现出较强的遵循指令和初步的推理能力。

-   **慎用模型**: **Llama** 系列模型在 RL 训练中可能表现不佳。其主要问题在于，其基座模型在预训练阶段可能没有充分暴露于“思维链”(Chain-of-Thought, CoT) 或类似的多步推理数据。因此，强行通过 RL 来激发其 CoT 能力，可能会产生奇怪或不连贯的输出。相比之下，Qwen 等模型在后训练 (Post-Training) 阶段已经大量接触过类似 CoT 的数据，使其“思维模式”更接近于一个推理模型，从而更容易通过 RL 进行强化。

-   **冷启动策略**: 无论选择哪个模型，一个标准的最佳实践是：**Base Model → SFT → RL**。即先对基座模型进行监督微调 (Supervised Fine-Tuning, SFT)，使其初步具备任务所需的能力和输出格式，然后再通过 RL 进行能力的强化和探索。直接从 Base Model 开始 RL 训练通常非常困难且效率低下。

## 3.2. “思维链” (Thinking) 模型的模板复杂性

近年来，许多先进的开源模型（如 Qwen3, Kimi）都采用了“思维链”或“工具调用”(Tool-use) 的模式，即在生成最终答案之前，会先生成一个包含思考过程、推理步骤或工具调用信息的中间文本。这些模型在后训练时，往往采用了复杂的上下文管理和模板规则，直接对其进行进一步的 RL 训练充满了“陷阱”。

### 3.2.1. 上下文修改规则

一个核心的复杂性来源是，这些模型在多轮对话中的上下文并非简单拼接，而是经过了**动态修改**。

-   **示例：Qwen3 的上下文修改**
    -   **Turn 1**: 用户提问 → 模型生成 `<think>`...`</think>` (思考过程) → 模型生成最终答案。
    -   **Turn 2**: 用户继续提问 → 输入给模型的上下文**不是** Turn 1 的完整记录，而是经过修改的：Turn 1 的 `<think>`...`</think>` 部分被**删除**，只保留用户的原始问题和模型的最终答案。

-   **对标准 RL 算法的破坏**: 这种上下文修改规则完全破坏了标准多轮 RL 算法（如 PPO/GRPO）的假设。标准算法将整个多轮对话视为一个完整的轨迹 (trajectory)，梯度从轨迹的末尾向开头反向传播。但在 Qwen3 的范式下，Turn 2 的状态 $s_2$ 并不是从 Turn 1 的最终状态 $s_1$ 自然演变而来的，其历史信息被修改了。这导致：
    -   **轨迹不连贯**: 训练过程被分解为一系列上下文被修改的“单轮对话”，无法形成一个统一的马尔可夫决策过程 (MDP)。
    -   **梯度无法传递**: 无法将 Turn 2 的奖励信号通过 BPTT (Back-Propagation Through Time) 传递回 Turn 1 的决策中。

**结论**: 如果我们不了解一个 Thinking 模型在原始训练时所使用的确切 RL 算法和上下文修改规则，直接对其进行标准的 RL 后训练，极有可能因为“水土不服”而失败。

### 3.2.2. 多次工具调用的复杂模板

当模型在单轮对话中进行多次工具调用时，情况变得更加复杂。

-   **示例：Kimi 的工具调用模板**
    -   **Turn 1**: 用户提问 → 模型生成 `<think>`...`</think>` → 模型调用工具 A → 模型根据 A 的返回结果生成 `<think>`...`</think>` → 模型调用工具 B → ... → 模型生成最终答案。
    -   **Turn 2**: 输入给模型的上下文中，Turn 1 的所有 `<think>`...`</think>` 部分可能都会被删除，只保留工具调用的结果和最终答案。

这种极其复杂的模板使得通用的 RL 框架难以适用。每一个模型家族都可能有自己独特的、未公开的模板和训练范式。

## 3.3. 实践中的工程考虑

### 3.3.1. `<think>` 标签的 Tokenization

在选择或设计思维标签（如 `<think>`）时，需要考虑其在模型词汇表中的表示。

-   **问题**: 如果 `<think>` 这个字符串在 Tokenizer 中被分割成多个独立的 token（例如，`<`, `think`, `>`），那么模型在生成时很难稳定地输出这个完整的标签。它可能会生成 `<think` 或 `< think` 等变体。
-   **解决方案**: 应该选择一个在词汇表中被表示为**单个独立 token** 的特殊字符串作为思维标签，或者在 SFT 阶段通过大量数据教会模型稳定地生成这个多-token 标签。这被称为“词表亲和的 (vocabulary-friendly)” token。

### 3.3.2. 温度 (Temperature) 的设置

-   **问题**: 开源模型通常会给出推荐的推理超参数（如 `temperature=0.7`, `top_p=0.9`）。在 RL 训练的 rollout 阶段，我们是否应该遵循这些设置？
-   **推荐实践**:
    -   **训练 (Rollout)**: 建议将 `temperature` 设置为 1.0 或官方推荐的较高值。这可以增加采样的随机性，促进策略的**探索 (exploration)**。
    -   **评估 (Inference)**: 在评估模型性能时，应严格遵循官方推荐的确定性更高的设置（如 `temperature=0.1` 或 `greedy search`），以保证评估结果的稳定性和可复现性。

## 3.4. 总结

对 LLM 进行 RL 后训练，远不止是选择一个算法然后开始训练那么简单。它需要深入到模型和数据的细节中，进行细致的“模板工程”和“模型考古”。在开始一个项目前，必须仔细研究基座模型的特性、其可能的训练范式和模板规则，否则大量的计算资源很可能因为这些看似微小但致命的细节而被浪费。

---

### 参考文献

1.  Qwen Team. (2024). Qwen2.5 Technical Report. *arXiv preprint*.
2.  Wei, J., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems, 35*, 24824-24837.
