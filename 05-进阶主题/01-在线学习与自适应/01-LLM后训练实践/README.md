# LLM 后训练实践指南

**Authors:** Damon Li

---

本章节汇集了在大型语言模型 (LLM) 的强化学习 (RL) 后训练过程中的核心挑战、实践经验与理论分析。与纯粹的理论研究不同，本章更侧重于将理论应用于实际工程时遇到的具体问题和解决方案，旨在为研究者和工程师提供一份可操作的实践指南。

内容主要围绕 RL for LLM 的两大核心议题——**探索效率**和**训练稳定性**——展开，并深入探讨了算法、模型与工程实践中的各种“坑”与心得。

## 核心主题

1.  **[探索效率与 Off-Policy 训练](./01-探索效率与Off-Policy训练.md)**
    -   **核心问题**: 如何在 LLM 高昂的采样成本和巨大的动作空间下进行有效探索？
    -   **内容概要**: 
        -   On-Policy vs. Off-Policy 训练的利弊权衡。
        -   **重要性采样 (Importance Sampling)** 的数学原理及其在 PPO/GRPO 中的应用。
        -   实践中遇到的 logprob 精度问题和上下文依赖问题的解决方案。
        -   异步训练与优先级经验回放 (PER) 在提升探索效率中的作用。

2.  **[训练稳定性与算法选择](./02-训练稳定性与算法选择.md)**
    -   **核心问题**: 为何 LLM 的 RL 训练如此脆弱，以及如何提升其稳定性？
    -   **内容概要**: 
        -   从数学上分析巨大动作空间下“混沌的概率转移”问题。
        -   离群值与超长轨迹对训练的破坏性影响。
        -   **Sequence-Level Loss (GSPO)** 与 **Token-Level Loss (GRPO, DAPO)** 的优劣对比与选择策略。
        -   在负样本占主导时，如何通过**正样本加权**来稳定训练。
        -   PPO 中 **Critic 模型**的局限性，以及何时应避免使用 PPO。

3.  **[基座模型与模板工程](./03-基座模型与模板工程.md)**
    -   **核心问题**: 如何选择合适的基座模型，以及如何处理复杂的“思维链”模型模板？
    -   **内容概要**: 
        -   不同基座模型（Qwen, Mistral, Llama）的 RL 训练适应性分析。
        -   标准的 **SFT → RL** 冷启动策略。
        -   揭示“思维链”模型中复杂的**上下文修改规则**及其对标准 RL 算法的破坏。
        -   在工程实践中处理特殊 Token 和**温度 (Temperature)** 设置的最佳实践。

通过本章的学习，读者可以对 RL for LLM 后训练的全貌有一个更深刻、更贴近实践的理解，从而在自己的研究和工程项目中避免常见的陷阱，提高训练的成功率。
