# 2. 训练稳定性与算法选择

**Authors:** Damon Li

---

训练稳定性是 RL for LLM 面临的另一个核心难题。与可以平滑扩展 (scaling) 的预训练和 SFT 不同，RL 训练过程极其脆弱，常常在几千步内就因指标异常而崩溃。本节将深入探讨导致不稳定的关键因素，并分析不同算法选择对稳定性的影响。

## 2.1. 训练不稳定的根源

### 2.1.1. 巨大的动作空间与混沌的概率转移

传统 RL 任务（如 Atari 游戏）的动作空间通常很小（几十个离散动作）。当算法抑制一个错误动作的概率时，这部分概率会自然地、相对可控地分配到其他少数几个动作上。

然而，LLM 的动作空间是其整个词汇表（数万个 token），并且是序列化的。当 RL 算法试图通过低奖励来抑制一个不好的序列输出时，其影响是**混沌且不可预测的**。

-   **数学解释**: 假设我们有一个不好的序列 $a_{1:L}$，我们希望降低其概率 $\pi(a_{1:L}|s) = \prod_{t=1}^L \pi(a_t|s, a_{<t})$。通过策略梯度更新，我们实际上是在降低序列中每个 token 的条件概率 $\pi(a_t|s, a_{<t})$。在 Softmax 分布下，降低一个 token 的概率，会导致其概率质量被重新分配到词汇表中所有其他数万个 token 上。这种微小的、弥散性的概率转移在序列的每一步累积，最终可能导致模型生成完全意想不到的、甚至更糟糕的序列。**因此，在 LLM RL 中，仅仅抑制负样本是远远不够的，必须更加强调对正样本的学习。**

### 2.1.2. 离群值与超长轨迹的破坏性影响

RL 训练对离群值（outliers）极其敏感。在 LLM 中，模型有时会生成超长的、无意义的循环或重复序列。这些轨迹在计算 token-level 的损失时，会产生巨大的影响。

-   **数学解释**: 假设一个正常的轨迹长度为 $L_{norm}$，而一个崩溃的轨迹长度为 $L_{crash} \gg L_{norm}$。在计算一个 batch 的总损失时，这个崩溃轨迹的贡献会被其长度 $L_{crash}$ 放大。如果其优势函数 $A_t$ 也较大，它可能会主导整个 batch 的梯度，将策略参数推向一个非常糟糕的方向，从而引发训练崩溃。

-   **解决方案**: 必须对 rollout 的输出进行严格的监控和过滤。可以设置最大输出 token 长度，并对轨迹的奖励、优势值、KL 散度等指标进行离群值检测，及时过滤掉异常样本。

## 2.2. Loss 函数的选择与权衡

对于序列生成任务，损失函数可以在序列级别 (sequence-level) 或 token 级别 (token-level) 定义，这导致了不同的算法变体。

### 2.2.1. Sequence-Level Loss (如 GSPO)

-   **代表算法**: GSPO (Generalized Sequence-level Policy Optimization) [1]
-   **核心思想**: 将整个生成序列视为一个“宏动作”，在序列级别计算损失。GSPO 的目标函数可以看作是：

    $$
    L_{GSPO}(\theta) = -\log \sigma(\beta (R_{gen} - R_{ref}))
    $$

    其中 $R_{gen}$ 是当前策略生成序列的总奖励，$R_{ref}$ 是参考策略（或经验池中的其他样本）的奖励，$\sigma$ 是 sigmoid 函数，$\beta$ 是一个超参数。这类似于一个对比学习的目标，鼓励生成比参考样本更高奖励的序列。

-   **优点**:
    -   **更稳定**: 由于在序列级别进行比较和更新，避免了 token 级别噪声和混沌的概率转移问题，通常收敛更稳定。
    -   **对 MoE 模型友好**: GSPO 对 MoE (Mixture of Experts) 模型的负载均衡有优化，是训练 MoE 模型的首选。

-   **缺点**:
    -   **收敛偏慢**: 将整个序列作为一个单位，损失信号不够精细，可能导致学习效率较低。

### 2.2.2. Token-Level Loss (如 GRPO, DAPO)

-   **代表算法**: GRPO (Generalized Reinforcement Policy Optimization), DAPO (Direct Advantage Policy Optimization) [2]
-   **核心思想**: 将损失分解到每个 token 上，利用 token 级别的优势信息进行更新。这与标准的 PPO/TRPO 思想一致。

-   **GRPO (PPO)**: 使用标准的 PPO Clipped Objective，是应用最广泛的基线。

-   **DAPO**: DAPO 提出了一种直接的优势估计方法，并对长序列的优势分配进行了限制，认为在长序列的后期，优势信号可能不可靠。

-   **优点**:
    -   **收敛较快**: Token 级别的信源分配更精细，理论上学习效率更高。

-   **缺点**:
    -   **更易崩溃**: 如前所述，token 级别的更新更容易受到噪声、离群值和混沌概率转移的影响。
    -   **DAPO 对长序列的限制**: 使其在需要长序列生成能力的多轮对话等场景下可能表现不佳。

**实践建议**: 对于 Dense 模型，可以从 GRPO/PPO 开始尝试；如果遇到稳定性问题，切换到 GSPO 可能是一个更稳健的选择。对于 MoE 模型，应优先考虑 GSPO。

## 2.3. 正负样本的平衡策略

鉴于 LLM RL 中抑制负样本的困难性，如何有效利用和强调正样本变得至关重要。

-   **问题**: 如果模型在某类任务上成功率很低（例如，低于 10%），那么在随机采样的 batch 中，大部分样本都是负样本。如果直接用 GRPO/PPO 训练，负样本的梯度将占据主导地位，很容易导致模型“学废”，即为了避免惩罚而选择输出空内容或重复一些无意义的 token。

-   **解决方案**: 必须在损失函数中提高正样本的权重。
    1.  **样本级别过滤/加权**: 在一个 batch 中，识别出成功的轨迹（正样本），并给予它们更高的权重。例如，可以直接在计算损失时，将正样本的损失乘以一个大于 1 的系数。
    2.  **Token 级别过滤**: 在正样本轨迹中，并非所有 token 的贡献都是正的。可以只对那些优势函数 $A_t > 0$ 的 token 计算策略梯度损失。
    3.  **提升正样本的优势 (Advantage Weighting)**: 直接对正样本轨迹中的优势函数 $A_t$ 进行加权，放大其影响。

## 2.4. Critic 模型与 PPO 的局限性

PPO 算法依赖于一个 Critic 模型来估计状态值函数 $V(s)$，并计算优势函数 $A(s,a) = R(s,a) + \gamma V(s	) - V(s)$。然而，在 LLM 的复杂任务中，Critic 的准确性是一个巨大挑战。

-   **问题**: Critic 模型本身需要通过回归损失（如 MSE）来训练，以拟合真实的累积回报。但在主观性强、奖励稀疏或数据有冲突的任务中，Critic 很难学到一个准确的值函数。一个不准确的 Critic 会提供错误的优势信号，严重误导 Actor 的更新，导致训练不稳定。

-   **实践建议**: **当奖励是可验证的、客观的时候，应优先使用不依赖 Critic 的算法**，如 REINFORCE 或其变体 GRPO（在 GRPO 中，优势可以直接用 Monte Carlo 回报减去一个基线来计算，而无需学习一个 Critic）。只有在处理主观性任务（如对齐、风格模仿），需要从偏好数据中学习一个奖励模型或 Critic 时，才应使用 PPO。

---

### 参考文献

1.  Zhao, S., et al. (2024). Generalized Sequence-level Policy Optimization for Language Models. *arXiv preprint arXiv:2405.12193*.
2.  Yang, L., et al. (2024). When is Sequence-level Training Better than Token-level Training?. *arXiv preprint arXiv:2405.12193*.
