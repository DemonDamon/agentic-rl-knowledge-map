---

# 1.4 时间差分学习 (Temporal-Difference Learning)

---

## 1. 概述 (Overview)

时间差分 (Temporal-Difference, TD) 学习是强化学习中最为核心和新颖的思想之一 [1]。它巧妙地结合了动态规划 (DP) 的自举 (bootstrapping) 特性和蒙特卡罗 (MC) 方法的无模型 (model-free) 学习能力，从而在效率和适用性上取得了显著的平衡。TD 学习是现代大多数强化学习算法的基石。

TD 学习的核心思想是，在每一步交互后，利用当前获得的奖励和对下一状态值的估计来更新当前状态的值，而无需等待整个经验片段结束。其更新目标，即 **TD 目标 (TD Target)**，是 $R_{t+1} + \gamma V(S_{t+1})$，而 **TD 误差 (TD Error)** $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ 则驱动了学习过程。这种从一个估计值来更新另一个估计值的方式，就是所谓的“自举”。

本节将深入探讨 TD 学习的原理及其两种主要的控制算法：SARSA 和 Q-Learning。

1.  **TD 预测 (TD Prediction):** 这是 TD 学习最简单的形式，用于在给定策略 $\pi$ 的情况下估计其值函数 $V^\pi$。我们将从最基础的 **TD(0)** 算法入手，分析其与 MC 方法在偏差-方差权衡上的区别。此外，我们还将介绍 **TD(λ)**，它通过资格迹统一了 TD 和 MC 方法，提供了一个平滑调节自举程度的机制。

2.  **SARSA:** 这是一种在策略 (on-policy) TD 控制算法。其名称来源于其更新规则所依赖的五元组 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$。SARSA 直接学习动作值函数 $Q^\pi(s, a)$，并遵循当前策略（通常是 ε-贪婪策略）进行探索和改进。我们将分析其收敛条件，并介绍其与资格迹结合的 **SARSA(λ)** 算法。

3.  **Q-Learning:** 作为强化学习中最著名的算法之一，Q-Learning 是一种离策略 (off-policy) TD 控制算法 [2]。它直接学习最优动作值函数 $Q^*(s, a)$，而无需关心生成经验的行为策略是什么。其更新规则中的贪婪选择使得它能够从次优的探索性策略中学习到最优策略。我们将提供其收DENIED
