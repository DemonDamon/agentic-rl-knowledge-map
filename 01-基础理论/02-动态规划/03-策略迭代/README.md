---

# 1.2.3 策略迭代 (Policy Iteration)

---

## 1. 概述 (Overview)

策略迭代是一种经典的动态规划算法，用于在已知马尔可夫决策过程 (MDP) 模型的情况下求解最优策略 $\pi^*$ [1]。该算法的核心思想是遵循一个被称为**广义策略迭代 (Generalized Policy Iteration, GPI)** 的通用模式：从一个任意的初始策略开始，交替执行**策略评估 (Policy Evaluation)** 和**策略改进 (Policy Improvement)** 两个步骤，直至策略收敛到最优。

1.  **策略评估**: 对当前的策略 $\pi$ 计算其值函数 $V^\pi$。这一步通常通过迭代策略评估算法完成，直至值函数收敛。
2.  **策略改进**: 基于计算出的值函数 $V^\pi$，通过贪婪地选择动作来生成一个更好的新策略 $\pi'$。

这个过程不断重复，产生一个策略序列 $\pi_0, \pi_1, \pi_2, \dots$ 和一个值函数序列 $V^{\pi_0}, V^{\pi_1}, V^{\pi_2}, \dots$。根据策略改进定理，我们知道这个策略序列是单调不减的，即 $V^{\pi_{k+1}} \geq V^{\pi_k}$。由于在一个有限 MDP 中，策略的数量是有限的，因此这个过程必然会在有限步内收敛到一个最优策略 $\pi^*$。

策略迭代的优点是其收敛性有严格的理论保证，并且在许多情况下，它能比值迭代更快地找到最优策略，尤其是在策略空间远小于状态空间时。然而，它的缺点是每一步策略评估本身就是一个需要多次迭代的计算过程，可能会非常耗时。

本节将详细介绍策略迭代的完整算法流程，提供其收敛性的严格数学证明，并分析其计算复杂度。

## 2. 目录 (Table of Contents)

- [**01-算法流程**](./01-算法流程/README.md): 策略迭代的具体实现步骤和伪代码。
- [**02-收敛性证明**](./02-收敛性证明/README.md): 证明算法为何能保证收敛到最优策略。
- [**03-复杂度分析**](./03-复杂度分析/README.md): 分析算法的时间和空间复杂度。

## 3. 核心参考文献 (Core References)

1.  Howard, R. A. (1960). *Dynamic programming and Markov processes*. MIT press.
2.  Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press. (Section 4.3)
