---

# 1.2.4.3 值迭代与策略迭代的比较 (Comparison of Value Iteration and Policy Iteration)

---

## 1. 概述 (Overview)

值迭代 (Value Iteration, VI) 和策略迭代 (Policy Iteration, PI) 是动态规划中用于求解马尔可夫决策过程 (MDP) 的两种基本算法。它们都保证在模型已知的情况下能够找到最优策略，但它们实现这一目标的方式和计算特性有所不同。理解它们之间的差异对于选择合适的算法以及理解更高级的强化学习方法至关重要。

-   **策略迭代** 在策略空间中进行搜索，交替进行完整的策略评估和策略改进，直至策略稳定。
-   **值迭代** 在值函数空间中进行搜索，直接迭代贝尔曼最优性方程，直至值函数收敛。

本节将从多个维度对这两种算法进行深入的比较。

## 2. 核心思想与算法流程对比

| 对比维度 | 策略迭代 (Policy Iteration) | 值迭代 (Value Iteration) |
| :--- | :--- | :--- |
| **核心思想** | 交替进行两个独立的阶段：**评估**当前策略，然后**改进**该策略。 | 将评估和改进融合在一步中，直接迭代寻找**最优值函数**。 |
| **算法流程** | `π₀ → (E) → V^π₀ → (I) → π₁ → (E) → V^π₁ → ... → π*` | `V₀ → V₁ → V₂ → ... → V* → (Extract) → π*` |
| **更新规则** | **评估**: $V_{k+1} = B^\pi V_k$ (迭代至收敛)<br>**改进**: $\pi_{k+1} = \text{greedy}(V^{\pi_k})$ | $V_{k+1} = B^* V_k$ (直接迭代) |
| **策略表示** | 在每次迭代中都显式地存储和更新策略 $\pi_k$。 | 在迭代过程中不维护显式策略，仅在最后一步提取。 |

- **(E)** 代表策略评估 (Evaluation), **(I)** 代表策略改进 (Improvement)。

## 3. 计算复杂度与收敛性

| 对比维度 | 策略迭代 (Policy Iteration) | 值迭代 (Value Iteration) |
| :--- | :--- | :--- |
| **单次迭代复杂度** | **高**。包含一个完整的、多轮的策略评估内循环，复杂度为 $O(N_{PE} \cdot |\mathcal{S}|^2 |\mathcal{A}|)$ 或 $O(|\mathcal{S}|^3)$。 | **低**。每次迭代只是一次对所有状态的单次扫描，复杂度为 $O(|\mathcal{S}|^2 |\mathcal{A}|)$。 |
| **迭代次数** | **少**。通常在很少的几次迭代后，策略就会收敛到最优。 | **多**。通常需要更多的迭代次数才能使值函数收敛到足够精确的程度。 |
| **收敛性** | 策略在**有限步**内收敛到最优策略 $\pi^*$。 | 值函数**渐近收敛**到最优值函数 $V^*$。 |
| **终止条件** | 策略不再发生变化 ($"policy-stable"$ is true)。 | 值函数的变化量小于一个阈值 $\theta$。 |

## 4. 总结与选择

-   **策略迭代** 的主要计算开销在于其内部的策略评估步骤，该步骤本身就是一个完整的迭代过程。然而，它的主循环（改进策略的次数）通常非常少。
-   **值迭代** 的每次迭代都很快，但它可能需要大量的迭代才能使值函数收敛。

**如何选择？**

-   在许多问题中，两种算法的性能相似。然而，当策略空间远小于状态空间时，或者当一个好的初始策略已知时，**策略迭代**可能更快收敛，因为它可能只需要几次策略改进就能找到最优解。
-   对于状态空间非常大的问题，**值迭代**通常是更好的选择，因为它避免了策略评估中昂贵的内循环。它的每次迭代计算量是确定的，更容易实现和分析。

**联系:**

值迭代可以被看作是策略迭代的一个特例，其中策略评估步骤只进行**一次**迭代。这种“不精确”的评估思想催生了**修正策略迭代 (Modified Policy Iteration)**，它允许在策略评估中只进行有限次数的迭代，从而在两种算法之间进行权衡。

最终，这两种算法都为现代强化学习提供了基础。策略迭代的“评估-改进”模式是广义策略迭代 (GPI) 的核心，而值迭代的“贝尔曼最优性备份”思想则是 Q-Learning 等无模型算法的灵感来源。

## 5. 参考文献 (References)

1.  Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press. (Section 4.5)
2.  Puterman, M. L. (2014). *Markov decision processes: discrete stochastic dynamic programming*. John Wiley & Sons. (Chapter 6)
