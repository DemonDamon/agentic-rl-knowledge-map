# 1. 信息熵与策略熵：从信息论到强化学习

**Authors:** Damon Li

---

## 1.1. 信息熵 (Information Entropy)

信息熵，或称香农熵，是衡量一个随机变量不确定性的核心指标。一个随机变量的熵越大，其不确定性就越大，包含的信息量也越多。

### 1.1.1. 形式化定义

对于一个离散随机变量 $X$，其可能取值的集合为 $\mathcal{X} = \{x_1, x_2, \dots, x_n\}$，对应的概率质量函数为 $P(X=x_i) = p(x_i)$。$X$ 的信息熵 $H(X)$ 定义为：

$$
H(X) = -\sum_{i=1}^{n} p(x_i) \log_b p(x_i)
$$

其中：
- $p(x_i)$ 是事件 $x_i$ 发生的概率。
- $\log_b$ 是对数函数，底数 $b$ 的选择决定了熵的单位。
  - 当 $b=2$ 时，单位是比特 (bits)。
  - 当 $b=e$ 时，单位是奈特 (nats)。在机器学习和深度学习中，通常使用自然对数。

该公式可以理解为“一个事件所含信息量的期望值”。单个事件 $x_i$ 的信息量（或称自信息）定义为 $I(x_i) = -\log p(x_i)$，它表示当得知这个小概率事件发生时，我们获得了多少信息。因此，熵是所有可能事件信息量的概率加权平均：

$$
H(X) = \mathbb{E}_{X \sim p} [I(X)] = \mathbb{E}_{X \sim p} [-\log p(X)]
$$

### 1.1.2. 主要性质

1.  **非负性**: $H(X) \ge 0$。因为 $0 \le p(x_i) \le 1$，所以 $\log p(x_i) \le 0$，从而 $-p(x_i) \log p(x_i) \ge 0$。
2.  **确定性**: 如果一个事件是确定的，即 $p(x_k)=1$ 且对所有 $i \neq k$ 有 $p(x_i)=0$，则 $H(X) = 0$。这符合直觉：一个确定的系统没有任何不确定性。
3.  **最大熵**: 当所有事件等可能发生时，即 $p(x_i) = 1/n$ 对所有 $i$ 成立，熵达到最大值 $H(X) = \log n$。这表示均匀分布具有最大的不确定性。

## 1.2. 策略熵 (Policy Entropy)

在强化学习 (RL) 的语境下，熵的概念被用来量化一个智能体（Agent）策略的不确定性或随机性，这被称为**策略熵**。

### 1.2.1. 形式化定义

一个策略 $\pi$ 定义了在给定状态 $s$ 下，选择每个可能动作 $a$ 的概率分布 $\pi(a|s)$。因此，在特定状态 $s$ 下的策略熵 $H(\pi(\cdot|s))$ 为：

$$
H(\pi(\cdot|s)) = -\sum_{a \in \mathcal{A}} \pi(a|s) \log \pi(a|s)
$$

其中 $\mathcal{A}$ 是动作空间。

- **高策略熵**：表示在状态 $s$ 下，策略对多个动作的倾向比较均匀，不确定接下来该做什么。这有利于**探索 (Exploration)**，即尝试新的、可能带来更高回报的动作。
- **低策略熵**：表示在状态 $s$ 下，策略高度倾向于选择一个或少数几个动作，对其决策非常“自信”。这有利于**利用 (Exploitation)**，即执行当前已知的最优动作。

### 1.2.2. 在大型语言模型 (LLM) 中的具体化

当我们将 LLM 视为一个强化学习智能体时，上述概念可以被具体化：

- **状态 (State, $s$)**: 在自回归生成任务中，状态是当前已经生成的所有上文 tokens，例如 $(t_1, t_2, \dots, t_{k-1})$。
- **动作 (Action, $a$)**: 动作是模型从其词汇表 $\mathcal{V}$ 中选择的下一个要生成的 token $t_k$。
- **策略 (Policy, $\pi$)**: LLM 本身就是一个策略函数。它接收状态 $s$（上文），输出一个在整个词汇表 $\mathcal{V}$ 上的概率分布，即 $\pi(a|s) = P(t_k=a | t_1, \dots, t_{k-1})$。这个分布通常是通过对模型的 logits 输出应用 Softmax 函数得到的。

因此，在给定上文 $s$ 的情况下，LLM 的策略熵为：

$$
H(\pi(\cdot|s)) = -\sum_{a \in \mathcal{V}} \pi(a|s) \log \pi(a|s)
$$

### 1.2.3. 期望策略熵

在实际训练和分析中，我们通常更关心策略在整个任务数据分布上的平均不确定性。这通过计算**期望策略熵**来实现，即对所有可能状态 $s$ 的策略熵求期望：

$$
\mathcal{H}(\pi) = \mathbb{E}_{s \sim d^\pi} [H(\pi(\cdot|s))]
$$

其中 $d^\pi$ 是在策略 $\pi$ 下的状态访问分布。在实践中，这个期望值通常通过在一个 mini-batch 的样本上计算平均熵来近似。

## 1.3. 熵在强化学习中的作用

策略熵在 RL 训练中扮演着至关重要的角色，尤其是在平衡探索与利用的权衡中。

- **熵奖励 (Entropy Bonus)**: 许多现代 RL 算法（如 Soft Actor-Critic, SAC）会在目标函数中直接加入一个熵奖励项，以鼓励智能体维持较高的策略熵，从而促进探索。
  
  $$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t) \right] + \alpha \mathcal{H}(\pi_\theta)$$
  
  其中 $\alpha$ 是控制熵奖励重要性的温度系数。

- **熵崩溃 (Entropy Collapse)**: 正如相关研究指出的，在没有有效干预的情况下，LLM 进行 RL 训练时，策略熵会过早地急剧下降至接近零。这种“熵崩溃”现象会导致模型过早地停止探索，陷入次优解，从而限制了其性能的进一步提升，形成“熵瓶颈”。

理解策略熵的数学定义及其在 RL 中的作用，是分析和解决“熵崩溃”问题的第一步。

---

### 参考文献

1.  Shannon, C. E. (1948). A mathematical theory of communication. *Bell System Technical Journal, 27*(3), 379-423.
2.  Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
